{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.pyimagesearch.com/2016/01/11/opencv-panorama-stitching/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "from functools import reduce\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in_interval(o, w0, w1, h0, h1):\n",
    "    '''\n",
    "    o is a tuple like (79,153). w0, w1, h0, h1 defines a square.\n",
    "    if o in the square, return True\n",
    "    '''\n",
    "    return (o[0] >= w0) and (o[0] <= w1) and (o[1] >= h0) and (o[1] <= h1)\n",
    "\n",
    "\n",
    "def grid_pick(querykeys,trainkeys):\n",
    "#         '''\n",
    "#         input:feature points(querykeys, trainkeys)\n",
    "#         output:selected points(querykeys, trainkeys)\n",
    "#         原理；对参与拼接的第二张图（因为第一张图有可能是多图拼的，图像区域不规则）\n",
    "#         划分grid_szw*grid_szh网格，在不同格子里提取(trainkeys)特征点，然后对应选出\n",
    "#         第一张图的相应点，最后返回筛选后的querykeys, trainkeys\n",
    "#         '''\n",
    "    grid_szh = 5\n",
    "    grid_szw = 5\n",
    "    RESIZE_H=1000\n",
    "    RESIZE_W=750\n",
    "    inter_h = [(RESIZE_H // grid_szh) * i for i in range(grid_szh)]\n",
    "    inter_h.append(RESIZE_H)\n",
    "    inter_w = [(RESIZE_W // grid_szw) * i for i in range(grid_szw)]\n",
    "    inter_w.append(RESIZE_W)\n",
    "    res_train = list()\n",
    "    res_query = list()\n",
    "    tmplist = np.zeros([grid_szh, grid_szw, 2])  # 记录像素点的位置flag\n",
    "    for i in range(grid_szw):\n",
    "        for j in range(grid_szh):\n",
    "            for o in trainkeys:  # 下面的判断保证每个方格只取一个点\n",
    "                if ((in_interval(o, inter_w[i], inter_w[i + 1], inter_h[j], inter_h[j + 1])) and (\n",
    "                        np.array(tmplist[j][i]).all() == np.array([0, 0]).all())):\n",
    "                    res_train.append(o)\n",
    "                    tmplist[j][i] = o\n",
    "                    res_query.append(querykeys[trainkeys.index(o)])  # 如果trainkeys的某格子里有一点，把对应位置querykeys也加上\n",
    "    return (res_query, res_train)\n",
    "\n",
    "###########################\n",
    "#下面都是从panorama.py里复制的函数\n",
    "\n",
    "def make_mask(target, src):\n",
    "    CommonMaskRGB = np.zeros((target.shape[0], target.shape[1]), dtype=np.uint8)\n",
    "    SrcMaskRGB = np.zeros((target.shape[0], target.shape[1]), dtype=np.uint8)\n",
    "    TargetMaskRGB = np.zeros((target.shape[0], target.shape[1]), dtype=np.uint8)\n",
    "    CommonMaskRGB[(cv2.cvtColor(target,cv2.COLOR_RGB2GRAY) != 0) * (cv2.cvtColor(src,cv2.COLOR_RGB2GRAY) != 0)] = 255\n",
    "    SrcMaskRGB[(cv2.cvtColor(src,cv2.COLOR_RGB2GRAY) != 0) * (CommonMaskRGB == 0)] = 255\n",
    "    TargetMaskRGB[(cv2.cvtColor(target,cv2.COLOR_RGB2GRAY) != 0)] = 255\n",
    "    CommonMask = cv2.erode(CommonMaskRGB,np.ones((5,5),np.uint8),iterations = 3)\n",
    "    SrcMask = cv2.dilate(SrcMaskRGB,np.ones((5,5),np.uint8),iterations = 1)\n",
    "    TargetMask = cv2.dilate(TargetMaskRGB,np.ones((3,3),np.uint8),iterations = 1)\n",
    "    return CommonMask, SrcMask, TargetMask\n",
    "\n",
    "\n",
    "def calc_dst4points(H, size):\n",
    "    x = []\n",
    "    y = []\n",
    "    x.append(((H[0][0]*0 + H[0][1]*0 + H[0][2])/(H[2][0]*0 + H[2][1]*0 + H[2][2])))\n",
    "    y.append(((H[1][0]*0 + H[1][1]*0 + H[1][2])/(H[2][0]*0 + H[2][1]*0 + H[2][2])))\n",
    "    x.append(((H[0][0]*0 + H[0][1]*size[0] + H[0][2])/(H[2][0]*0 + H[2][1]*size[0] + H[2][2])))\n",
    "    y.append(((H[1][0]*0 + H[1][1]*size[0] + H[1][2])/(H[2][0]*0 + H[2][1]*size[0] + H[2][2])))\n",
    "    x.append(((H[0][0]*size[1] + H[0][1]*0 + H[0][2])/(H[2][0]*size[1] + H[2][1]*0 + H[2][2])))\n",
    "    y.append(((H[1][0]*size[1] + H[1][1]*0 + H[1][2])/(H[2][0]*size[1] + H[2][1]*0 + H[2][2])))\n",
    "    x.append(((H[0][0]*size[1] + H[0][1]*size[0] + H[0][2])/(H[2][0]*size[1] + H[2][1]*size[0] + H[2][2])))\n",
    "    y.append(((H[1][0]*size[1] + H[1][1]*size[0] + H[1][2])/(H[2][0]*size[1] + H[2][1]*size[0] + H[2][2])))\n",
    "\n",
    "    min_x = min(x)\n",
    "    min_y = min(y)\n",
    "    max_x = max(x)\n",
    "    max_y = max(y)\n",
    "    div = ((min_x, max_x),(min_y, max_y))\n",
    "    return div\n",
    "\n",
    "def resize_mat(image, div):\n",
    "    height, width = image.shape[0:2]\n",
    "    d = [0, 0, width, height]\n",
    "    if div[0][0] < 0:\n",
    "        d[0] = div[0][0]\n",
    "    if div[0][1] > width:\n",
    "        d[2] = div[0][1]\n",
    "    if div[1][0] < 0:\n",
    "        d[1] = div[1][0]\n",
    "    if div[1][1] > height:\n",
    "        d[3] = div[1][1]\n",
    "    T = np.array([[1.0, 0.0, -d[0]], [0.0, 1.0, -d[1]], [0.0, 0.0, 1.0]])\n",
    "    image = cv2.warpPerspective(image, T, (int(-d[0] + d[2]), int(-d[1] + d[3])))\n",
    "    return (image,d)\n",
    "\n",
    "\n",
    "def get_center(mask):\n",
    "    min_x = 10000\n",
    "    max_x = -1\n",
    "    min_y = 10000\n",
    "    max_y = -1\n",
    "    for y in range(mask.shape[0]):\n",
    "        for x in range(mask.shape[1]):\n",
    "            if(mask[y][x]):\n",
    "                if(x<min_x):\n",
    "                    min_x = x\n",
    "                if(y<min_y):\n",
    "                    min_y = y\n",
    "                if(x>max_x):\n",
    "                    max_x = x\n",
    "                if(y>max_y):\n",
    "                    max_y = y\n",
    "    return (max_y+min_y)/2, (max_x+min_x)/2\n",
    "\n",
    "\n",
    "def arrange_rgb(mat, TargetMask):\n",
    "    mat[TargetMask==0] = [0,0,0]\n",
    "    gray = cv2.cvtColor(mat,cv2.COLOR_RGB2GRAY)\n",
    "    mat[(TargetMask != 0) * (gray == 0)] = 1\n",
    "    return mat\n",
    "\n",
    "def write_blending(target, source, SrcMask):\n",
    "    mask = cv2.cvtColor(SrcMask,cv2.COLOR_GRAY2RGB)\n",
    "    target[(mask != [0,0,0])] = source[(mask != [0,0,0])]\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stitcher:\n",
    "    def __init__(self):\n",
    "        # determine if we are using OpenCV v3.X\n",
    "        self.isv3 = imutils.is_cv3()\n",
    "\n",
    "    def stitch(self, images, ratio=0.75, reprojThresh=4.0,\n",
    "               showMatches=False):\n",
    "        # unpack the images, then detect keypoints and extract\n",
    "        # local invariant descriptors from them\n",
    "        (imageB, imageA) = images\n",
    "        (kpsA, featuresA) = self.detectAndDescribe(imageA)\n",
    "        (kpsB, featuresB) = self.detectAndDescribe(imageB)\n",
    "\n",
    "        # match features between the two images\n",
    "        M = self.matchKeypoints(kpsA, kpsB,\n",
    "                                featuresA, featuresB, ratio, reprojThresh)\n",
    "\n",
    "        # if the match is None, then there aren't enough matched\n",
    "        # keypoints to create a panorama\n",
    "        if M is None:\n",
    "            return None\n",
    "\n",
    "        # otherwise, apply a perspective warp to stitch the images\n",
    "        # together\n",
    "        (matches, H, status) = M\n",
    "        \n",
    "        #result = cv2.warpPerspective(imageA, H,\n",
    "         #                            (imageA.shape[1] + imageB.shape[1], imageA.shape[0] + imageB.shape[0]))\n",
    "     #########################\n",
    "        div = calc_dst4points(H, imageA.shape) #original1.image-->imageB, original2..-->imageA, imageB.image-->imageB\n",
    "        imageB,d = resize_mat(imageB,div)        \n",
    "        T_xy = [[1.0, 0.0, -d[0]],[0.0, 1.0, -d[1]],[0.0, 0.0, 1.0]]\n",
    "        panorama = cv2.warpPerspective(imageA,np.dot(T_xy,H),(imageB.shape[1],imageB.shape[0]))\n",
    "        CommonMask, SrcMask, TargetMask = make_mask(panorama, imageB)\n",
    "        label = cv2.connectedComponentsWithStats(CommonMask)\n",
    "        center = np.delete(label[3], 0, 0)\n",
    "        test = get_center(CommonMask)\n",
    "        blending = cv2.seamlessClone(imageB, panorama, cv2.cvtColor(CommonMask,cv2.COLOR_GRAY2BGR), (int(test[1]),int(test[0])), cv2.NORMAL_CLONE)\n",
    "        blending = arrange_rgb(blending, TargetMask)\n",
    "        blending = write_blending(blending, imageB, SrcMask)\n",
    "        result=blending\n",
    "        #result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    "#         for i in range(imageB.shape[0]):  #这里改写了，原代码result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    "#             for j in range(imageB.shape[1]):\n",
    "#                 if not (imageB[i][j] == [0, 0, 0]).all():\n",
    "#                     result[i][j] = imageB[i][j]  #result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    "\n",
    "        # check to see if the keypoint matches should be visualized\n",
    "        if showMatches:\n",
    "            vis = self.drawMatches(imageA, imageB, kpsA, kpsB, matches,  #\n",
    "                                   status)\n",
    "\n",
    "            # return a tuple of the stitched image and the\n",
    "            # visualization\n",
    "            return (result, vis, H)  #########这里改动\n",
    "\n",
    "        # return the stitched image\n",
    "        return (result, H)\n",
    "\n",
    "\n",
    "    def detectAndDescribe(self, image):\n",
    "        # convert the image to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # check to see if we are using OpenCV 3.X\n",
    "        if self.isv3:\n",
    "            # detect and extract features from the image\n",
    "            descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "            (kps, features) = descriptor.detectAndCompute(image, None)\n",
    "\n",
    "        # otherwise, we are using OpenCV 2.4.X\n",
    "        else:\n",
    "            # detect keypoints in the image\n",
    "            detector = cv2.FeatureDetector_create(\"SIFT\")\n",
    "            kps = detector.detect(gray)\n",
    "\n",
    "            # extract features from the image\n",
    "            extractor = cv2.DescriptorExtractor_create(\"SIFT\")\n",
    "            (kps, features) = extractor.compute(gray, kps)\n",
    "\n",
    "        # convert the keypoints from KeyPoint objects to NumPy\n",
    "        # arrays\n",
    "        kps = np.float32([kp.pt for kp in kps])\n",
    "\n",
    "        # return a tuple of keypoints and features\n",
    "        return (kps, features)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def matchKeypoints(self, kpsA, kpsB, featuresA, featuresB,\n",
    "                       ratio, reprojThresh):\n",
    "        # compute the raw matches and initialize the list of actual\n",
    "        # matches\n",
    "         # FLANN parameters这一段是flann match,换一个match\n",
    "#         FLANN_INDEX_KDTREE = 0\n",
    "#         index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "#         search_params = dict(checks=50)   # or pass empty dictionary\n",
    "#         matcher = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "        ##################\n",
    "        matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "        \n",
    "        rawMatches = matcher.knnMatch(featuresA, featuresB, 2)\n",
    "        matches = []\n",
    "        #         matches = sorted(matches, key = lambda x:x.distance)\n",
    "        # loop over the raw matches\n",
    "        for m in rawMatches:\n",
    "            # ensure the distance is within a certain ratio of each\n",
    "            # other (i.e. Lowe's ratio test)\n",
    "            if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
    "                matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "        # computing a homography requires at least 4 matches\n",
    "        if len(matches) > 4:\n",
    "            # construct the two sets of points\n",
    "            ptsA = np.float32([kpsA[i] for (_, i) in matches[:10]])  #这里可以对matches特征数量加以限制，如matches[:100]\n",
    "            ptsB = np.float32([kpsB[i] for (i, _) in matches[:10]])\n",
    "        \n",
    "        ######下面这段网格筛选\n",
    "#             ptsA=ptsA.tolist()\n",
    "#             ptsB=ptsB.tolist()\n",
    "#             ptsA,ptsB = grid_pick(ptsA,ptsB)\n",
    "#             ptsA,ptsB = np.array(ptsA), np.array(ptsB)\n",
    "            #compute the homography between the two sets of points\n",
    "            (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,\n",
    "                                             reprojThresh)\n",
    "\n",
    "            # return the matches along with the homograpy matrix\n",
    "            # and status of each matched point\n",
    "            return (matches, H, status)\n",
    "\n",
    "        # otherwise, no homograpy could be computed\n",
    "        return None\n",
    "\n",
    "    def drawMatches(self, imageA, imageB, kpsA, kpsB, matches, status):\n",
    "        # initialize the output visualization image\n",
    "        (hA, wA) = imageA.shape[:2]\n",
    "        (hB, wB) = imageB.shape[:2]\n",
    "        vis = np.zeros((max(hA, hB), wA + wB, 3), dtype=\"uint8\")\n",
    "        vis[0:hA, 0:wA] = imageA\n",
    "        vis[0:hB, wA:] = imageB\n",
    "\n",
    "        # loop over the matches\n",
    "        for ((trainIdx, queryIdx), s) in zip(matches, status):\n",
    "            # only process the match if the keypoint was successfully\n",
    "            # matched\n",
    "            if s == 1:\n",
    "                # draw the match\n",
    "                ptA = (int(kpsA[queryIdx][0]), int(kpsA[queryIdx][1]))\n",
    "                ptB = (int(kpsB[trainIdx][0]) + wA, int(kpsB[trainIdx][1]))\n",
    "                cv2.line(vis, ptA, ptB, (0, 255, 0), 1)\n",
    "\n",
    "        # return the visualization\n",
    "        return vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gene1crop(pic1path,pic2path,height=1000):\n",
    "    imageB = cv2.imread(pic1path)\n",
    "    imageA = cv2.imread(pic2path)\n",
    "    #imageB = imutils.resize(imageB, height)\n",
    "    imageA = imutils.resize(imageA, height)\n",
    "    # stitch the images together to create a panorama\n",
    "    stitcher = Stitcher()\n",
    "    (result, vis,H) = stitcher.stitch([imageB, imageA], showMatches=True)\n",
    "\n",
    "    # show the images\n",
    "    # cv2.imshow(\"Image A\", imageA)\n",
    "    # cv2.imshow(\"Image B\", imageB)\n",
    "    #\n",
    "    #cv2.imshow(\"Result\", result)\n",
    "    #cv2.imwrite(\"0102bgtst.jpg\",result)\n",
    "    #按图像大小裁剪一个框，去掉框外黑区域\n",
    "    gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "    #gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    hmax=0\n",
    "    wmax=0\n",
    "    for i in range(gray.shape[0]):\n",
    "        for j in range(gray.shape[1]):\n",
    "            if gray[i][j]!=0:\n",
    "                if wmax<j:wmax=j\n",
    "                if hmax<i:hmax=i\n",
    "    cropImg = result[0:hmax, 0:wmax]\n",
    "    \n",
    "    return (cropImg,vis,H)\n",
    "\n",
    "def gene1img(pic1path,pic2path,height=1000): #pic1path是PosixPath对象，可以用.parts操作\n",
    "    cropImg,vis,H=gene1crop(str(pic1path),str(pic2path),height=height)\n",
    "    cv2.imwrite(\"test01.jpg\",cropImg)\n",
    "    cv2.imwrite(\"green01.jpg\",vis)\n",
    "    ###下面添加dump H矩阵代码，保存名字是dump_heat+第二张图的文件名\n",
    "    inname=pic2path.parts[-1].split(\".\")[0]\n",
    "    dumpname='dump'+inname+'.txt' #保存H矩阵\n",
    "    f = open(dumpname, 'wb')\n",
    "    pickle.dump(H, f)\n",
    "    f.close()\n",
    "    #######\n",
    "    return \"test01.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test01.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gene1img(\"./tmp/folderImg/010203_1000.jpg\",\"./tmp/folderImg/04.jpg\",height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imageT = imutils.resize('./bcropsamp.jpg', 800)\n",
    "cv2.imwrite(\"imageT.jpg\",imageT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gene1img('bcropsamp.jpg','04.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以上是函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('tmp/folderImg/notused/01.jpg'),\n",
       " PosixPath('tmp/folderImg/notused/02.jpg')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path('./')\n",
    "folderImg=PATH.glob('./tmp/folderImg/notused/*.jpg') #./tmp/folderImg3/*.jpg\n",
    "imglist=[i for i in folderImg]\n",
    "imglist=sorted(imglist)\n",
    "imglist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "images=os.listdir(folderImg)\n",
    "images=sorted(images)\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imglist[0].parts[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imageB = cv2.imread(str(imglist[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test01.jpg'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reduce(gene1img, imglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imageB = cv2.imread('tmp/folderImg1/01.jpg')\n",
    "# imageB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(imageB.smhape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imageB = cv2.imread('01.jpg')\n",
    "imageA = cv2.imread('02.jpg')\n",
    "imageB = imutils.resize(imageB, height=800)\n",
    "imageA = imutils.resize(imageA, height=800)\n",
    "# stitch the images together to create a panorama\n",
    "stitcher = Stitcher()\n",
    "(result, vis,H) = stitcher.stitch([imageB, imageA], showMatches=True)\n",
    " \n",
    "# show the images\n",
    "# cv2.imshow(\"Image A\", imageA)\n",
    "# cv2.imshow(\"Image B\", imageB)\n",
    "#\n",
    "#cv2.imshow(\"Result\", result)\n",
    "cv2.imwrite(\"samp.jpg\",result)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 把拼出的图像裁剪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "img = cv2.imread('010203bgtst.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "hmax=0\n",
    "wmax=0\n",
    "for i in range(gray.shape[0]):\n",
    "    for j in range(gray.shape[1]):\n",
    "        if gray[i][j]!=0:\n",
    "            if wmax<j:wmax=j\n",
    "            if hmax<i:hmax=i\n",
    "hmax,wmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cropImg = img[0:hmax, 0:wmax]\n",
    "cv2.imwrite(\"b.jpg\",cropImg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "imageB = cv2.imread('b.jpg')\n",
    "imageA = cv2.imread('04.jpg')\n",
    "imageB = imutils.resize(imageB, height=800)\n",
    "imageA = imutils.resize(imageA, height=800)\n",
    "# stitch the images together to create a panorama\n",
    "stitcher = Stitcher()\n",
    "(result, vis,H) = stitcher.stitch([imageB, imageA], showMatches=True)\n",
    " \n",
    "# show the images\n",
    "# cv2.imshow(\"Image A\", imageA)\n",
    "# cv2.imshow(\"Image B\", imageB)\n",
    "#\n",
    "cv2.imshow(\"Green lines\", vis)\n",
    "cv2.imwrite(\"01020304bgtstlines.jpg\",vis)\n",
    "#cv2.imshow(\"Result\", result)\n",
    "#cv2.imwrite(\"01020304bgtst.jpg\",result)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这里整合上面几段缝合代码，写循环，实现多图连拼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imageB = cv2.imread('010203bg001.jpg')\n",
    "imageA = cv2.imread('04.jpg')\n",
    "imageB = imutils.resize(imageB, height=800)\n",
    "imageA = imutils.resize(imageA, height=800)\n",
    "# stitch the images together to create a panorama\n",
    "stitcher = Stitcher()\n",
    "(result, vis,H) = stitcher.stitch([imageB, imageA], showMatches=True)\n",
    " \n",
    "# show the images\n",
    "# cv2.imshow(\"Image A\", imageA)\n",
    "# cv2.imshow(\"Image B\", imageB)\n",
    "#\n",
    "#cv2.imshow(\"Result\", result)\n",
    "cv2.imwrite(\"01020304bg001.jpg\",result)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "imageB1 = cv2.imread('bg1.jpg')\n",
    "imageA1 = cv2.imread('bg2.jpg')\n",
    "result1 = cv2.warpPerspective(imageA1, H,\n",
    "            (imageA1.shape[1] + imageB1.shape[1], imageA1.shape[0]))\n",
    "result1[0:imageB1.shape[0], 0:imageB1.shape[1]] = imageB1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.imwrite(\"0102bg1.jpg\",result1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36cv]",
   "language": "python",
   "name": "conda-env-py36cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 183,
   "position": {
    "height": "205px",
    "left": "805px",
    "right": "20px",
    "top": "108px",
    "width": "648px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
